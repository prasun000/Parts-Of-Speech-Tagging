{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP06R7xNNLrbgibD1g2pW1W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prasun000/Parts-Of-Speech-Tagging/blob/main/pos_tagging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# POS Tagging as a Classification Problem\n",
        "\n",
        "---\n",
        "We will apply SVM algorithm to solve this multiclass classification problem (one - vs - rest type SVM classifiers )."
      ],
      "metadata": {
        "id": "p_oZyiLoAhVJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsS6Yv4H6_DJ",
        "outputId": "3b746a73-560f-455d-cd69-8e3d7333a5a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download(\"treebank\")\n",
        "nltk.download(\"brown\")\n",
        "from nltk.corpus import  treebank"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(treebank.tagged_sents())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGuXd6zF0C1C",
        "outputId": "1e797b9c-3252-4b9d-c1a8-88b112666e0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3914"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We download the `treebank` corpus from `nltk` library as our data set. We wish to view it as a **multiclass classification** problem ( map words to pos labels)."
      ],
      "metadata": {
        "id": "Xqs_Bl5qhw2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(sentence, index, opt):\n",
        "  if (opt == 1) :\n",
        "    return {\n",
        "      'word':sentence[index],\n",
        "      'suffix-1':sentence[index][-1],\n",
        "      'suffix-2':sentence[index][-2:],\n",
        "      'suffix-3':sentence[index][-3:],\n",
        "      'prev_word':'' if index == 0 else sentence[index-1],\n",
        "      'next_word':'' if index == (len(sentence)-1) else sentence[index+1]\n",
        "  }\n",
        "  elif (opt == 2) :\n",
        "    return {\n",
        "      'word':sentence[index],\n",
        "      'is_first':index==0,\n",
        "      'is_last':index ==len(sentence)-1,\n",
        "      'is_capitalized':sentence[index][0].upper() == sentence[index][0],\n",
        "      'is_all_caps': sentence[index].upper() == sentence[index],\n",
        "      'is_all_lower': sentence[index].lower() == sentence[index],\n",
        "      'is_alphanumeric': sentence[index].isalnum(),\n",
        "      'prefix-1':sentence[index][0],\n",
        "      'prefix-2':sentence[index][:2],\n",
        "      'prefix-3':sentence[index][:3],\n",
        "      'prefix-3':sentence[index][:4],\n",
        "      'suffix-1':sentence[index][-1],\n",
        "      'suffix-2':sentence[index][-2:],\n",
        "      'suffix-3':sentence[index][-3:],\n",
        "      'prev_word':'' if index == 0 else sentence[index-1],\n",
        "      'next_word':'' if index == (len(sentence)-1) else sentence[index+1],\n",
        "      'has_hyphen': '-' in sentence[index],\n",
        "      'is_numeric': sentence[index].isdigit(),\n",
        "      'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
        "  }\n",
        "\n",
        "import pprint\n",
        "pprint.pprint(extract_features(['Hello', 'read3er'], index = 0, opt = 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5m8P4CAH7cT0",
        "outputId": "b83a7260-84c2-434c-cd4d-a868c2bce71b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'capitals_inside': False,\n",
            " 'has_hyphen': False,\n",
            " 'is_all_caps': False,\n",
            " 'is_all_lower': False,\n",
            " 'is_alphanumeric': True,\n",
            " 'is_capitalized': True,\n",
            " 'is_first': True,\n",
            " 'is_last': False,\n",
            " 'is_numeric': False,\n",
            " 'next_word': 'read3er',\n",
            " 'prefix-1': 'H',\n",
            " 'prefix-2': 'He',\n",
            " 'prefix-3': 'Hell',\n",
            " 'prev_word': '',\n",
            " 'suffix-1': 'o',\n",
            " 'suffix-2': 'lo',\n",
            " 'suffix-3': 'llo',\n",
            " 'word': 'Hello'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We do feature extraction from the words as follows :\n",
        "\n",
        "\n",
        "`word` $\\mapsto$ (`word`,`prev_word` , `next_word`, `suffix-1`, `suffix-2`, `suffix-3`)\n",
        "\n",
        "Note that this feature extraction is done for `opt=1`"
      ],
      "metadata": {
        "id": "RzSYlCOwiz6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_to_dataset(tagged_sentences, opt):\n",
        "    X, y = [], []\n",
        "    for sentence in tagged_sentences:\n",
        "        recover_sentence = [word[0] for word in sentence]\n",
        "        recover_tag      = [word[1] for word in sentence]\n",
        "        for word_index in range(len(sentence)):\n",
        "            X.append(extract_features(recover_sentence, word_index, opt = opt))\n",
        "            y.append(recover_tag[word_index])\n",
        "    return X, y\n",
        "\n",
        "opt = 2\n",
        "penn_train_size = int(0.8*len(treebank.tagged_sents())) # training size is 80% of tagged sents\n",
        "penn_training   = treebank.tagged_sents()[:penn_train_size]\n",
        "penn_testing    = treebank.tagged_sents()[penn_train_size:]\n",
        "X_penn_train, y_penn_train = transform_to_dataset(penn_training, opt = opt)\n",
        "X_penn_test , y_penn_test  = transform_to_dataset(penn_testing , opt = opt)"
      ],
      "metadata": {
        "id": "1SUQx9ufBSlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After splitting our data into training and testing , we store our data in a data-set format where each row of, say, `X_penn_train` is a list of dicionaries . Each dictionary contains the features for a particular word of a sentence. The lists in the dictionary encompass the words of one ( or maybe more ) consecutive sentence(s) appearing in the corpus."
      ],
      "metadata": {
        "id": "loPgeKP1lI0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "V2IjypBzMCVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we choose to represent features using **one-hot-encoding** as we are dealing with categorical features. We use this to construct `X_test` , `y_test` where the $i$-th row of `X_test` has rows that represent the encoded vectors for the $i$-th word of the training corpus. `y_test` refers to the corresponding labels."
      ],
      "metadata": {
        "id": "NbeCZWFvpAXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fit a DicVectorizer that learns the one-hot encoding on the training data\n",
        "v = DictVectorizer()\n",
        "X_train = v.fit_transform(X_penn_train)\n",
        "\n",
        "# Encode test data using fitted vectorizer\n",
        "X_test  = v.transform(X_penn_test)"
      ],
      "metadata": {
        "id": "c27s96roySBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TS9sfqem8_H0",
        "outputId": "56f8a1b6-b7b1-4e88-ae7c-da4b8d5fca21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<80637x42386 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 1451466 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logistic_reg = LogisticRegression().fit(X_train,y_penn_train)\n",
        "logistic_regPreds = logistic_reg.predict(X_test)\n",
        "print(classification_report(y_penn_test,logistic_regPreds,zero_division= 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAkzFZO5H4Lh",
        "outputId": "8acc524d-7025-4768-c476-abdcaeb55e2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           #       1.00      1.00      1.00         2\n",
            "           $       1.00      1.00      1.00       242\n",
            "          ''       1.00      1.00      1.00        78\n",
            "           ,       1.00      1.00      1.00       930\n",
            "       -LRB-       1.00      1.00      1.00        26\n",
            "      -NONE-       1.00      1.00      1.00      1340\n",
            "       -RRB-       1.00      1.00      1.00        26\n",
            "           .       1.00      1.00      1.00       762\n",
            "           :       1.00      1.00      1.00        77\n",
            "          CC       1.00      1.00      1.00       429\n",
            "          CD       1.00      1.00      1.00      1032\n",
            "          DT       0.99      0.99      0.99      1611\n",
            "          EX       0.88      1.00      0.93         7\n",
            "          IN       0.98      0.98      0.98      1952\n",
            "          JJ       0.86      0.88      0.87      1087\n",
            "         JJR       0.82      0.82      0.82        76\n",
            "         JJS       0.77      0.89      0.83        38\n",
            "          MD       0.99      1.00      1.00       204\n",
            "          NN       0.95      0.93      0.94      2901\n",
            "         NNP       0.93      0.96      0.95      1800\n",
            "        NNPS       0.71      0.38      0.49        64\n",
            "         NNS       0.94      0.99      0.97      1127\n",
            "         PDT       1.00      0.00      0.00         6\n",
            "         POS       0.98      1.00      0.99       196\n",
            "         PRP       1.00      1.00      1.00       222\n",
            "        PRP$       1.00      1.00      1.00       122\n",
            "          RB       0.91      0.88      0.89       478\n",
            "         RBR       0.42      0.31      0.36        16\n",
            "         RBS       0.60      0.60      0.60         5\n",
            "          RP       0.56      0.74      0.63        34\n",
            "          TO       1.00      1.00      1.00       464\n",
            "          VB       0.96      0.93      0.94       498\n",
            "         VBD       0.93      0.95      0.94       743\n",
            "         VBG       0.91      0.92      0.92       261\n",
            "         VBN       0.93      0.87      0.90       452\n",
            "         VBP       0.89      0.82      0.86       165\n",
            "         VBZ       0.98      0.90      0.94       324\n",
            "         WDT       0.99      0.99      0.99       104\n",
            "          WP       1.00      1.00      1.00        26\n",
            "         WP$       1.00      1.00      1.00         4\n",
            "         WRB       1.00      0.96      0.98        27\n",
            "          ``       1.00      1.00      1.00        81\n",
            "\n",
            "    accuracy                           0.96     20039\n",
            "   macro avg       0.93      0.90      0.90     20039\n",
            "weighted avg       0.96      0.96      0.96     20039\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "nb_clf = BernoulliNB(binarize = None).fit(X_train,y_penn_train)\n",
        "nbPreds = nb_clf.predict(X_test)\n",
        "print(classification_report(y_penn_test,nbPreds,zero_division= 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HQxKyL_2kTJ",
        "outputId": "ee397b09-0ceb-4f33-8217-67a41808f7f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           #       1.00      0.00      0.00         2\n",
            "           $       1.00      0.00      0.00       242\n",
            "          ''       1.00      0.99      0.99        78\n",
            "           ,       1.00      1.00      1.00       930\n",
            "       -LRB-       1.00      0.00      0.00        26\n",
            "      -NONE-       0.78      1.00      0.87      1340\n",
            "       -RRB-       1.00      0.00      0.00        26\n",
            "           .       1.00      1.00      1.00       762\n",
            "           :       1.00      0.00      0.00        77\n",
            "          CC       1.00      0.99      1.00       429\n",
            "          CD       0.98      0.90      0.94      1032\n",
            "          DT       0.98      0.99      0.98      1611\n",
            "          EX       1.00      0.00      0.00         7\n",
            "          IN       0.87      0.98      0.92      1952\n",
            "          JJ       0.69      0.69      0.69      1087\n",
            "         JJR       1.00      0.00      0.00        76\n",
            "         JJS       1.00      0.00      0.00        38\n",
            "          MD       1.00      0.00      0.00       204\n",
            "          NN       0.66      0.96      0.78      2901\n",
            "         NNP       0.83      1.00      0.91      1800\n",
            "        NNPS       1.00      0.00      0.00        64\n",
            "         NNS       0.83      0.96      0.89      1127\n",
            "         PDT       1.00      0.00      0.00         6\n",
            "         POS       1.00      0.00      0.00       196\n",
            "         PRP       1.00      0.92      0.96       222\n",
            "        PRP$       1.00      0.00      0.00       122\n",
            "          RB       1.00      0.57      0.73       478\n",
            "         RBR       1.00      0.00      0.00        16\n",
            "         RBS       1.00      0.00      0.00         5\n",
            "          RP       1.00      0.00      0.00        34\n",
            "          TO       1.00      1.00      1.00       464\n",
            "          VB       0.83      0.37      0.51       498\n",
            "         VBD       0.89      0.82      0.85       743\n",
            "         VBG       1.00      0.00      0.01       261\n",
            "         VBN       0.99      0.48      0.64       452\n",
            "         VBP       1.00      0.17      0.29       165\n",
            "         VBZ       0.51      0.59      0.55       324\n",
            "         WDT       1.00      0.00      0.00       104\n",
            "          WP       1.00      0.00      0.00        26\n",
            "         WP$       1.00      0.00      0.00         4\n",
            "         WRB       1.00      0.00      0.00        27\n",
            "          ``       1.00      0.99      0.99        81\n",
            "\n",
            "    accuracy                           0.83     20039\n",
            "   macro avg       0.95      0.41      0.42     20039\n",
            "weighted avg       0.86      0.83      0.79     20039\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "neigh = KNeighborsClassifier(n_neighbors=5).fit(X_train,y_penn_train)\n",
        "neighPreds = neigh.predict(X_test)\n",
        "print(classification_report(y_penn_test,neighPreds,zero_division= 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_doEdkZre6Cl",
        "outputId": "20987d91-e317-48e6-adf2-845b079a1fc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           #       1.00      1.00      1.00         2\n",
            "           $       1.00      1.00      1.00       242\n",
            "          ''       1.00      1.00      1.00        78\n",
            "           ,       1.00      1.00      1.00       930\n",
            "       -LRB-       1.00      1.00      1.00        26\n",
            "      -NONE-       1.00      1.00      1.00      1340\n",
            "       -RRB-       1.00      1.00      1.00        26\n",
            "           .       1.00      1.00      1.00       762\n",
            "           :       1.00      1.00      1.00        77\n",
            "          CC       0.99      1.00      1.00       429\n",
            "          CD       0.99      1.00      1.00      1032\n",
            "          DT       0.98      0.99      0.99      1611\n",
            "          EX       0.47      1.00      0.64         7\n",
            "          IN       0.97      0.98      0.98      1952\n",
            "          JJ       0.80      0.84      0.82      1087\n",
            "         JJR       0.79      0.84      0.82        76\n",
            "         JJS       0.81      0.89      0.85        38\n",
            "          MD       0.96      1.00      0.98       204\n",
            "          NN       0.90      0.88      0.89      2901\n",
            "         NNP       0.93      0.94      0.94      1800\n",
            "        NNPS       0.71      0.39      0.51        64\n",
            "         NNS       0.92      0.97      0.95      1127\n",
            "         PDT       0.00      0.00      0.00         6\n",
            "         POS       0.98      0.99      0.99       196\n",
            "         PRP       0.99      1.00      0.99       222\n",
            "        PRP$       0.99      0.99      0.99       122\n",
            "          RB       0.87      0.89      0.88       478\n",
            "         RBR       0.54      0.44      0.48        16\n",
            "         RBS       0.60      0.60      0.60         5\n",
            "          RP       0.59      0.65      0.62        34\n",
            "          TO       1.00      1.00      1.00       464\n",
            "          VB       0.79      0.76      0.77       498\n",
            "         VBD       0.85      0.83      0.84       743\n",
            "         VBG       0.86      0.84      0.85       261\n",
            "         VBN       0.73      0.75      0.74       452\n",
            "         VBP       0.88      0.62      0.73       165\n",
            "         VBZ       0.97      0.85      0.90       324\n",
            "         WDT       0.95      1.00      0.97       104\n",
            "          WP       0.96      1.00      0.98        26\n",
            "         WP$       1.00      1.00      1.00         4\n",
            "         WRB       1.00      0.93      0.96        27\n",
            "          ``       1.00      1.00      1.00        81\n",
            "\n",
            "    accuracy                           0.93     20039\n",
            "   macro avg       0.88      0.88      0.87     20039\n",
            "weighted avg       0.93      0.93      0.93     20039\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LinearSVCClassObj = LinearSVC().fit(X_train,y_penn_train) #fitting SVM one-vs-rest classifier\n",
        "LinearSVCPreds    = LinearSVCClassObj.predict(X_test)     #predicting labels using fitted classifier\n",
        "print(classification_report(y_penn_test,LinearSVCPreds,zero_division= 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzAq82hdzDNe",
        "outputId": "a0a67b3e-415c-4f9c-e36e-9eb06c6d703a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           #       1.00      1.00      1.00         2\n",
            "           $       1.00      1.00      1.00       242\n",
            "          ''       1.00      1.00      1.00        78\n",
            "           ,       1.00      1.00      1.00       930\n",
            "       -LRB-       1.00      1.00      1.00        26\n",
            "      -NONE-       1.00      1.00      1.00      1340\n",
            "       -RRB-       1.00      1.00      1.00        26\n",
            "           .       1.00      1.00      1.00       762\n",
            "           :       1.00      1.00      1.00        77\n",
            "          CC       1.00      1.00      1.00       429\n",
            "          CD       1.00      1.00      1.00      1032\n",
            "          DT       0.99      0.99      0.99      1611\n",
            "          EX       0.88      1.00      0.93         7\n",
            "          IN       0.98      0.98      0.98      1952\n",
            "          JJ       0.87      0.90      0.88      1087\n",
            "         JJR       0.82      0.86      0.84        76\n",
            "         JJS       0.88      0.95      0.91        38\n",
            "          MD       0.99      1.00      1.00       204\n",
            "          NN       0.95      0.94      0.95      2901\n",
            "         NNP       0.95      0.95      0.95      1800\n",
            "        NNPS       0.73      0.42      0.53        64\n",
            "         NNS       0.95      0.99      0.97      1127\n",
            "         PDT       0.67      0.33      0.44         6\n",
            "         POS       0.99      1.00      1.00       196\n",
            "         PRP       1.00      1.00      1.00       222\n",
            "        PRP$       1.00      1.00      1.00       122\n",
            "          RB       0.91      0.89      0.90       478\n",
            "         RBR       0.50      0.38      0.43        16\n",
            "         RBS       0.67      0.80      0.73         5\n",
            "          RP       0.60      0.76      0.68        34\n",
            "          TO       1.00      1.00      1.00       464\n",
            "          VB       0.94      0.94      0.94       498\n",
            "         VBD       0.94      0.95      0.94       743\n",
            "         VBG       0.92      0.92      0.92       261\n",
            "         VBN       0.91      0.88      0.90       452\n",
            "         VBP       0.86      0.84      0.85       165\n",
            "         VBZ       0.98      0.92      0.95       324\n",
            "         WDT       0.99      1.00      1.00       104\n",
            "          WP       0.96      1.00      0.98        26\n",
            "         WP$       1.00      1.00      1.00         4\n",
            "         WRB       1.00      1.00      1.00        27\n",
            "          ``       1.00      1.00      1.00        81\n",
            "\n",
            "    accuracy                           0.96     20039\n",
            "   macro avg       0.92      0.92      0.92     20039\n",
            "weighted avg       0.96      0.96      0.96     20039\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We fit a simple SVM classifier and report the results.\n",
        "\n",
        "We try out kernel SVM as well with the polynomial kernel and observe the following."
      ],
      "metadata": {
        "id": "zizucV6KqnxB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# POS Tagging as sequence to sequence labelling problem\n",
        "\n",
        "--------------------------------------------\n",
        "Now we will treat POS tagging as a seq2seq labelling problem, and apply hmm model."
      ],
      "metadata": {
        "id": "Od4LjWvmKFzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import HiddenMarkovModelTagger\n",
        "# penn_training is the training data\n",
        "hmm_tagger  = HiddenMarkovModelTagger.train(penn_training)\n",
        "print(hmm_tagger)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STKqliACKaYw",
        "outputId": "65165e37-ef03-475d-b537-fbb49363ea4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<HiddenMarkovModelTagger 46 states and 11044 output symbols>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hmm_ = [hmm_tagger.tag(sent) for sent in treebank.sents()[penn_train_size:]]\n",
        "# hmm_pred stores the predicted labels\n",
        "hmm_pred = []\n",
        "for tagged_sent in hmm_ :\n",
        "  for word_index in range(len(tagged_sent)):\n",
        "    hmm_pred.append(tagged_sent[word_index][1])"
      ],
      "metadata": {
        "id": "457feUmIoo4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_penn_test,hmm_pred, zero_division = 1))\n",
        "len(set(y_penn_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmb18IGmLRyy",
        "outputId": "fa0a6321-fd09-47c7-e383-99dc5c43289b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           #       1.00      1.00      1.00         2\n",
            "           $       0.76      1.00      0.86       242\n",
            "          ''       0.53      1.00      0.69        78\n",
            "           ,       0.96      1.00      0.98       930\n",
            "       -LRB-       1.00      0.88      0.94        26\n",
            "      -NONE-       0.94      1.00      0.97      1340\n",
            "       -RRB-       0.62      0.92      0.74        26\n",
            "           .       0.90      1.00      0.95       762\n",
            "           :       1.00      1.00      1.00        77\n",
            "          CC       0.97      1.00      0.99       429\n",
            "          CD       0.94      0.85      0.89      1032\n",
            "          DT       0.90      0.99      0.95      1611\n",
            "          EX       1.00      1.00      1.00         7\n",
            "          IN       0.97      0.98      0.97      1952\n",
            "          JJ       0.79      0.79      0.79      1087\n",
            "         JJR       0.80      0.87      0.84        76\n",
            "         JJS       1.00      0.84      0.91        38\n",
            "          LS       0.00      1.00      0.00         0\n",
            "          MD       0.92      1.00      0.96       204\n",
            "          NN       0.94      0.85      0.89      2901\n",
            "         NNP       0.92      0.80      0.85      1800\n",
            "        NNPS       0.50      0.09      0.16        64\n",
            "         NNS       0.87      0.86      0.86      1127\n",
            "         PDT       1.00      0.50      0.67         6\n",
            "         POS       0.93      0.97      0.95       196\n",
            "         PRP       0.65      0.99      0.79       222\n",
            "        PRP$       0.71      1.00      0.83       122\n",
            "          RB       0.86      0.84      0.85       478\n",
            "         RBR       0.80      0.25      0.38        16\n",
            "         RBS       0.67      0.80      0.73         5\n",
            "          RP       0.48      0.68      0.56        34\n",
            "          TO       0.92      1.00      0.96       464\n",
            "          VB       0.83      0.95      0.89       498\n",
            "         VBD       0.95      0.87      0.91       743\n",
            "         VBG       0.89      0.68      0.77       261\n",
            "         VBN       0.84      0.78      0.81       452\n",
            "         VBP       0.90      0.79      0.84       165\n",
            "         VBZ       0.91      0.88      0.90       324\n",
            "         WDT       0.90      1.00      0.95       104\n",
            "          WP       0.93      1.00      0.96        26\n",
            "         WP$       1.00      0.50      0.67         4\n",
            "         WRB       0.86      0.93      0.89        27\n",
            "          ``       0.67      0.99      0.80        81\n",
            "\n",
            "    accuracy                           0.90     20039\n",
            "   macro avg       0.84      0.86      0.82     20039\n",
            "weighted avg       0.91      0.90      0.90     20039\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "46"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}